{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic pytorch tutorial from [here](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize a tensor from a data. e.g. 2-d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1,2], [3, 4]]\n",
    "x_data = torch.tensor(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a tensor from a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a tensor from another tensor. The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones tensor: tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "\n",
      "random tensor: tensor([[0.0901, 0.4822],\n",
      "        [0.9089, 0.0780]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # retrin the properties of x_data\n",
    "print(f\"ones tensor: {x_ones}\\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # override the datatype of x_data\n",
    "print(f\"random tensor: {x_rand}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with a random or constant value. `shape` is a tuple of tensor dimention. In the functions below, ir determines the dimensionality of the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random tensor: tensor([[0.4272, 0.0437, 0.8781],\n",
      "        [0.4689, 0.9854, 0.6647]])\n",
      "ones tensor: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "zeros tensor: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2, 3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"random tensor: {rand_tensor}\")\n",
    "print(f\"ones tensor: {ones_tensor}\")\n",
    "print(f\"zeros tensor: {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor also have it's own attributes, it desctibes their shape, datatype and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random tensor: tensor([[0.2671, 0.6848, 0.9897, 0.4451],\n",
      "        [0.0690, 0.9546, 0.5507, 0.0236],\n",
      "        [0.6751, 0.8684, 0.4806, 0.7136]])\n",
      "shape of tensor: torch.Size([3, 4])\n",
      "data type of tensor: torch.float32\n",
      "device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "print(f\"random tensor: {tensor}\")\n",
    "print(f\"shape of tensor: {tensor.shape}\")\n",
    "print(f\"data type of tensor: {tensor.dtype}\")\n",
    "print(f\"device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor operations. tensors support multiple operations, including transposing, indexing, slicing, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device tensor is stored on mps:0\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    tensor = tensor.to('mps')\n",
    "    print(f\"device tensor is stored on {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# tensor support slicing and indexing\n",
    "tensor = torch.ones(4, 4)\n",
    "print(tensor)\n",
    "tensor[:, 1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# we can also concate multiple tensors\n",
    "\n",
    "t1 = torch.cat([tensor, tensor, tensor, tensor])\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor * tensor tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# this compute the element-wise product\n",
    "print(f\"tensor.mul(tensor) {tensor.mul(tensor)}\")\n",
    "# this is alternative syntax\n",
    "print(f\"tensor * tensor {tensor * tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`in-place operation` optnerations that have a `_` suffix are `in-place`. For example, `x.copy_(y)`, `x.t_()`, will change `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor)\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bridge with numpy\n",
    "tensors on the cpu and numpy arrays can share their underlaying memory locations, and changing one will change the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "# reflect to the t and n instance.\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([3., 3., 3., 3., 3.])\n",
      "n: [3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A gentle introduction to `torch.autograd`\n",
    "neuron networks are collection of nested functions that are executed on some input data. These functions are defined by `parameters` (consisting of weight and biases) which pytorch are stored in tensors.\n",
    "\n",
    "training a NN happens in two steps:\n",
    "\n",
    "## forward propagation\n",
    "\n",
    "in forward pp, the NN makes it's best guess about the correct output. it runs the input data through each of it's functions to make this guess.\n",
    "\n",
    "## backward propagation\n",
    "\n",
    "in back pp, the NN adjust it's parameters proportionate to the error in it's guess. It does this by traversing backwards from the output, collecting the derivatives of the error with respect to the parameters of the functions (`gradients`) and optimizing the parameters using `gradient descent`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start from an example\n",
    "we load a pretrained resnet18 model from `torchvision`. We create a random data tensor to represent a simple image with 3 channels, and height & width of 64 and it's coresponding `label` initialized to some random values. Labels in pretrained models has shape (1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "# data is represent as a 3 channel 64x64 image, and each channel'value type is float that from 0 to 1\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimention is torch.Size([1, 3, 64, 64])\n",
      "data datatype is torch.float32\n",
      "label dimention is torch.Size([1, 1000])\n",
      "lable  datatype is torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(f\"data dimention is {data.shape}\")\n",
    "print(f\"data datatype is {data.dtype}\")\n",
    "print(f\"label dimention is {labels.shape}\")\n",
    "print(f\"lable  datatype is {labels.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we have the resnet18 model with a default weight. Now we run `forward pass`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction is tensor([[-5.9637e-01, -4.0663e-01, -6.1672e-01, -1.6282e+00, -8.0112e-01,\n",
      "         -3.7188e-01, -6.1246e-01,  5.9014e-01,  4.3512e-01, -8.1981e-01,\n",
      "         -9.9065e-01, -5.0543e-01, -2.0572e-01, -7.6218e-01, -1.0108e+00,\n",
      "         -7.1315e-01, -7.6979e-01, -8.0279e-02, -3.8696e-01, -6.8331e-01,\n",
      "         -1.3993e+00, -6.6493e-01, -1.3539e+00,  1.9825e-01, -7.8628e-01,\n",
      "         -1.1046e+00, -6.1931e-01, -1.0785e+00, -7.9773e-01, -2.8110e-01,\n",
      "         -8.3119e-01, -9.6465e-01, -6.4080e-01, -7.5072e-01, -4.1383e-01,\n",
      "         -5.7549e-01,  5.1857e-01, -7.9591e-01, -4.1426e-01,  3.0670e-02,\n",
      "         -7.4567e-01, -8.4535e-01, -1.0504e+00, -4.6258e-01, -5.7252e-01,\n",
      "         -3.8199e-01, -7.2758e-01, -4.2273e-01, -1.2770e+00, -1.3260e+00,\n",
      "         -5.4800e-01,  2.5394e-01, -1.7533e-01, -4.0595e-01, -4.6777e-02,\n",
      "         -8.8195e-01, -1.3289e-01, -1.1557e+00, -1.9127e-01, -2.5418e-01,\n",
      "          7.8824e-01,  4.8648e-02, -9.1774e-02,  3.9039e-01, -4.9240e-01,\n",
      "          9.2045e-02, -1.3418e-01, -2.5437e-01, -8.0560e-01, -1.2394e+00,\n",
      "         -1.6541e+00,  3.8439e-02, -1.3560e+00, -3.4242e-01, -1.0728e+00,\n",
      "         -1.3092e+00,  2.1810e-01, -6.0152e-01,  3.9708e-01,  1.9199e-01,\n",
      "         -7.4494e-01, -1.4893e+00,  1.0344e-01, -7.7033e-01, -6.0577e-01,\n",
      "          1.2871e-01,  2.8002e-01,  4.4383e-01,  1.4907e-01, -3.8831e-01,\n",
      "         -8.1968e-01, -1.0507e+00, -1.9211e+00, -2.4094e-01,  4.4521e-02,\n",
      "         -2.2049e+00, -4.4216e-01, -3.5414e-01, -1.3171e+00, -5.0802e-01,\n",
      "         -1.0290e+00, -1.1706e+00, -8.6787e-01, -1.4785e-01, -3.2597e-01,\n",
      "         -6.0880e-01, -4.4302e-01, -1.3939e+00, -1.0328e+00, -1.4088e+00,\n",
      "         -1.1640e+00, -5.6047e-01,  1.1448e+00,  4.7496e-01,  3.5240e-01,\n",
      "         -9.5736e-01, -8.6314e-01, -1.5947e-01,  6.3800e-01, -2.7288e-01,\n",
      "         -5.7045e-01,  1.3958e-01,  4.6912e-01,  4.3632e-02,  1.2792e+00,\n",
      "          8.5516e-02,  4.1242e-01, -1.1533e+00, -1.0104e+00, -1.0267e+00,\n",
      "         -1.4397e+00, -1.3499e+00, -9.8431e-01, -1.5408e+00, -8.0181e-01,\n",
      "         -1.2045e+00, -8.2446e-01, -1.3194e+00, -1.4827e+00, -1.2655e+00,\n",
      "         -1.5479e+00, -1.6391e+00, -2.3454e+00, -1.2734e+00, -6.1692e-01,\n",
      "         -3.2848e-01, -6.0564e-01, -1.6918e+00, -1.4060e+00, -1.2938e+00,\n",
      "          4.9741e-01,  1.5582e+00, -7.6048e-01, -2.1244e-01,  2.7516e-01,\n",
      "          4.0068e-01, -1.3512e-01, -2.1883e-01,  2.0988e-01,  2.7519e-01,\n",
      "          2.9906e-01,  7.5353e-01,  2.4067e-01,  6.2997e-01,  2.3035e-01,\n",
      "         -1.1671e-01, -1.8309e-01, -4.8526e-01,  5.0111e-01, -1.7679e-01,\n",
      "          8.6668e-03,  9.1817e-01,  4.1542e-01,  1.8122e-01,  1.7390e-01,\n",
      "         -7.0490e-01,  1.7532e-01,  1.6411e-01,  7.0768e-01,  6.9798e-01,\n",
      "          5.8669e-01,  1.3328e-01,  8.3935e-01,  2.4865e-02,  6.5012e-01,\n",
      "          1.0002e+00,  6.5830e-01,  3.7165e-01,  3.3910e-01,  7.9257e-01,\n",
      "         -4.1541e-01,  7.6263e-01,  2.5144e-01,  7.3135e-01, -7.1897e-01,\n",
      "          6.6901e-01,  2.3286e-01,  1.8774e-01,  2.3542e-01,  6.5846e-01,\n",
      "          3.1574e-01,  3.8594e-01,  5.4973e-01,  4.5706e-01,  3.7162e-01,\n",
      "          4.6712e-01, -2.1776e-02,  6.5721e-01,  1.3254e+00,  6.1588e-01,\n",
      "         -6.4685e-02,  5.4476e-01,  3.5793e-01,  1.9941e-01,  2.2707e-01,\n",
      "          4.3271e-01,  7.8152e-02,  4.0002e-01, -2.2372e-01,  5.8024e-01,\n",
      "          2.9175e-01, -4.2229e-01, -7.0581e-02,  6.7620e-01,  3.3772e-01,\n",
      "          2.3705e-01,  5.5537e-02,  6.4838e-01, -5.8057e-01, -2.5603e-02,\n",
      "          4.6731e-02,  3.0236e-01,  5.0545e-01, -1.3813e-01,  8.4538e-01,\n",
      "          4.9947e-01,  5.4329e-01,  3.8718e-01,  1.0486e+00,  7.4007e-02,\n",
      "          8.2060e-01,  7.6033e-02,  2.8503e-01,  3.4950e-01, -6.6176e-02,\n",
      "          2.8976e-01,  3.2287e-01,  4.3176e-03,  5.2240e-01,  1.1675e-02,\n",
      "          3.9657e-01,  3.7696e-01, -5.5266e-01,  5.1520e-01,  1.1339e+00,\n",
      "         -6.2817e-01,  5.0292e-01,  2.8790e-01, -1.8419e-01,  2.1776e-01,\n",
      "         -2.3635e-01, -5.2753e-01,  2.5141e-02,  1.8939e-01,  5.1698e-01,\n",
      "          6.9971e-01,  2.9732e-01,  5.0341e-01, -3.4999e-02, -5.6194e-01,\n",
      "         -1.0160e+00, -1.3625e+00, -7.6573e-01,  5.0192e-01, -1.2769e+00,\n",
      "         -1.2754e+00, -1.3846e+00, -1.2027e+00, -1.7052e+00, -4.7019e-01,\n",
      "         -7.3472e-01,  8.2444e-01,  6.0658e-01,  2.1396e-01,  1.9847e-01,\n",
      "          7.3938e-01, -4.0418e-01, -4.2110e-01, -8.3497e-01, -1.7934e+00,\n",
      "         -1.1253e+00, -9.7956e-01, -5.0457e-01, -1.2152e+00, -8.5250e-01,\n",
      "         -9.3883e-01, -9.4740e-01, -1.3504e+00, -4.4180e-01, -2.7435e-01,\n",
      "         -1.9329e+00, -7.6932e-01, -3.0087e-01, -3.9823e-01, -1.1849e+00,\n",
      "         -8.7721e-01,  3.4266e-02, -7.9878e-01, -1.3680e+00, -2.2914e-01,\n",
      "          3.5373e-01, -4.0422e-01, -3.5614e-01,  9.7515e-02,  8.3676e-01,\n",
      "         -3.7510e-01, -9.6878e-01, -1.0079e+00, -1.3043e+00, -7.9968e-01,\n",
      "         -1.4826e+00, -1.0983e+00, -1.5301e+00, -1.6713e+00, -1.4966e+00,\n",
      "         -1.6214e+00, -1.2929e+00, -7.2818e-02, -1.6171e-01, -7.0964e-01,\n",
      "         -2.4366e-01, -5.4234e-01,  6.0282e-03,  3.4602e-01, -5.6054e-01,\n",
      "         -9.8648e-01, -1.4324e+00,  2.5184e-01,  1.0583e+00, -1.4668e+00,\n",
      "         -5.9650e-01,  6.1328e-01, -5.5258e-01, -1.5320e+00, -9.9763e-01,\n",
      "          7.4639e-01, -5.3501e-01, -1.8944e+00,  1.2357e-02, -1.3223e+00,\n",
      "         -9.6555e-01, -2.5135e+00, -1.6889e+00, -1.0819e+00, -8.4785e-01,\n",
      "          2.6859e-01,  1.0828e+00,  1.1882e-01,  5.1692e-01,  2.4886e-01,\n",
      "          5.8615e-02,  5.4953e-01, -4.0401e-02,  2.0577e-01, -5.1477e-01,\n",
      "         -3.0214e-01, -8.6357e-01, -7.1031e-02, -9.9719e-01, -4.8630e-01,\n",
      "         -5.3657e-01, -3.1757e-01, -3.7711e-01,  2.1291e-01, -1.2458e-01,\n",
      "         -7.7991e-01, -1.2182e+00,  4.3316e-01, -3.2390e-01, -4.1555e-01,\n",
      "          3.3799e-01, -3.3961e-01, -2.7092e-01, -6.5138e-01, -9.3873e-01,\n",
      "         -5.9772e-01, -1.3534e+00, -1.2019e+00, -9.4616e-01, -1.7743e-01,\n",
      "          8.3370e-01,  5.5193e-02, -1.2926e+00, -1.7002e+00, -3.7354e-01,\n",
      "          3.3809e-01, -1.1458e+00, -9.7589e-01,  5.0517e-01, -8.8325e-02,\n",
      "         -8.7109e-01,  7.4706e-01,  2.3505e-01, -2.1889e+00, -1.8705e+00,\n",
      "         -5.7857e-01, -1.1417e-01, -8.1544e-01, -5.1085e-01,  1.0246e+00,\n",
      "         -2.0678e-01,  1.9883e-01,  2.2326e+00,  5.3958e-01,  3.5928e-01,\n",
      "          6.3242e-01, -1.1519e-01,  3.5552e-01,  3.1349e-01,  9.1086e-01,\n",
      "          5.5934e-01,  1.1998e+00,  1.5829e-02,  1.6982e-01,  2.3149e-01,\n",
      "         -8.4646e-01,  1.1632e-01,  1.1490e+00,  1.7459e+00,  4.6885e-01,\n",
      "         -8.4697e-01, -3.1062e-02,  3.3041e-01,  7.0995e-01,  7.0402e-01,\n",
      "          1.1419e+00, -4.7713e-01, -6.0107e-02,  6.5538e-01,  1.6616e-01,\n",
      "          1.2114e+00,  4.2686e-01,  2.0653e-01, -4.2799e-01, -5.0847e-01,\n",
      "         -1.5318e-02,  5.4914e-01,  1.7864e+00,  8.7535e-01, -6.2077e-01,\n",
      "         -7.8050e-02,  5.5189e-02,  4.7007e-01, -5.9675e-02, -2.7275e-01,\n",
      "          6.2494e-01,  1.3391e+00,  1.2183e+00, -3.4239e-02,  6.4387e-01,\n",
      "         -7.9113e-01,  2.0118e-01,  1.2655e+00,  2.6466e+00,  1.0169e+00,\n",
      "         -5.9298e-01, -1.3448e+00, -1.1593e-01, -3.3469e-01,  1.4097e+00,\n",
      "          1.2452e+00,  5.6664e-01,  7.1022e-02,  1.0775e+00, -6.1468e-01,\n",
      "         -2.0403e-01, -1.1627e-01,  3.8043e-01,  8.7412e-01,  3.8707e-01,\n",
      "         -3.2091e-01,  2.0187e-01,  4.0207e-01, -9.0354e-01, -1.5657e+00,\n",
      "          2.6092e-03, -5.9004e-01,  1.4111e+00,  1.5223e+00,  9.7001e-01,\n",
      "          1.7039e-01,  6.6862e-01,  7.5542e-01, -1.3962e+00,  1.2586e+00,\n",
      "         -1.0659e+00,  1.2137e-01, -4.2928e-01, -2.2194e-01,  1.1903e+00,\n",
      "         -1.7161e+00,  3.9139e-01,  1.2063e+00,  6.6173e-01,  1.0247e+00,\n",
      "          1.1759e+00,  1.1610e+00,  8.3143e-01,  5.5148e-01,  1.5192e-01,\n",
      "         -1.0578e+00, -1.0083e+00,  9.9857e-01,  3.0405e-01,  9.8312e-01,\n",
      "          1.5077e+00,  4.8618e-01,  2.7346e-01,  1.3906e+00,  8.0931e-01,\n",
      "         -4.5623e-01,  3.3437e-01,  1.1356e+00,  1.6001e+00, -9.9579e-03,\n",
      "         -6.9783e-01,  5.5683e-02, -3.9552e-01,  6.7452e-01,  6.5190e-02,\n",
      "          9.2349e-01,  3.6315e-01,  1.2770e-01, -1.1205e+00,  2.7083e-01,\n",
      "         -9.4179e-01, -4.7771e-01, -8.5953e-01,  1.9102e-01,  1.2796e+00,\n",
      "         -1.0968e+00,  1.4369e+00,  9.1840e-01,  8.3354e-01,  6.5066e-01,\n",
      "          8.8659e-01,  5.6651e-01, -2.2077e+00, -1.2181e+00,  3.0986e-02,\n",
      "         -4.4345e-01, -8.8773e-03,  6.0716e-01, -7.2582e-02, -1.4387e+00,\n",
      "         -8.0229e-01,  2.0442e-01,  6.2308e-01,  1.2011e+00,  8.0799e-01,\n",
      "         -3.1578e-01, -2.8456e-01,  4.5669e-01,  3.9972e-01, -1.4134e+00,\n",
      "         -8.5525e-01,  1.6253e-01,  1.0878e+00,  1.1772e-01, -5.0662e-01,\n",
      "          1.3878e+00,  5.0381e-02,  7.3988e-01, -7.1932e-01,  6.7436e-01,\n",
      "         -5.4322e-01, -8.6309e-01,  1.1635e+00,  3.6282e-01,  1.8488e-01,\n",
      "          5.1117e-01, -1.7304e-01,  8.8405e-01,  7.2209e-01,  1.1429e+00,\n",
      "          9.8256e-01, -4.8725e-01,  1.8411e+00,  1.3186e+00,  1.4850e+00,\n",
      "         -5.1302e-01,  8.0520e-01, -6.0290e-01,  1.0165e+00,  3.1555e-01,\n",
      "         -3.8839e-01,  1.1444e+00, -4.3179e-01, -6.5313e-01,  7.3907e-01,\n",
      "          2.1840e+00, -3.8498e-01, -1.1638e-01, -7.3714e-01,  5.6855e-01,\n",
      "          2.7560e-01,  1.2334e+00, -5.6682e-01,  7.6682e-01, -4.0478e-01,\n",
      "          1.0859e+00,  8.9043e-01, -5.5047e-01,  5.1561e-01,  1.1913e-01,\n",
      "          2.2779e-01,  1.1979e+00,  5.3455e-01,  1.9590e+00,  8.9882e-01,\n",
      "          1.1023e+00,  7.7496e-01,  3.5390e-01,  4.0421e-01,  2.7032e-01,\n",
      "         -1.4960e+00,  1.5430e+00, -4.2470e-01, -1.1853e+00,  3.4915e-01,\n",
      "         -8.6049e-02,  1.0905e+00,  6.0967e-01,  1.2808e+00, -3.6916e-01,\n",
      "          3.7929e-01,  1.1827e+00,  1.0507e+00,  6.1255e-01, -8.5511e-02,\n",
      "         -1.8273e+00,  9.7389e-01, -3.3846e-01,  1.3858e+00,  7.9151e-01,\n",
      "         -8.4333e-01,  6.3683e-01,  7.3183e-01, -7.3134e-01, -1.5884e+00,\n",
      "          1.2169e+00, -2.3362e-02,  3.1096e-01,  8.7736e-01, -2.4137e-01,\n",
      "          3.9998e-01, -2.0992e-01,  7.3259e-02, -2.5287e-02,  3.3263e-01,\n",
      "         -1.6506e-02, -9.6561e-01, -1.1036e-01, -8.4764e-01,  1.0144e+00,\n",
      "         -7.6298e-02,  1.3775e+00,  4.1813e-01, -7.8328e-01, -2.8947e-01,\n",
      "          1.6247e-01, -3.3265e-01, -9.0226e-02,  7.4604e-01,  1.6543e+00,\n",
      "         -1.0531e+00,  1.7488e+00,  1.0657e+00,  8.2270e-01,  3.5248e-01,\n",
      "          6.5742e-01,  6.5048e-01, -9.2776e-01,  6.1016e-01,  7.0051e-01,\n",
      "         -1.3544e+00, -7.2285e-02, -1.0525e+00, -4.6423e-01, -1.1872e+00,\n",
      "         -6.5933e-01,  8.6518e-01,  1.0002e+00,  5.7990e-01, -9.1403e-01,\n",
      "          9.4885e-01,  1.7784e+00,  7.6600e-02, -5.5634e-01,  5.7266e-01,\n",
      "          2.0897e+00, -1.9007e-01, -3.4872e-01,  4.4701e-01,  3.7194e-01,\n",
      "         -4.6853e-01, -4.5296e-01,  4.0479e-01,  7.1513e-01,  2.7948e-01,\n",
      "          1.1393e+00,  1.0252e+00,  7.5769e-02, -5.0467e-01,  4.3248e-01,\n",
      "         -5.7602e-01,  6.0671e-01, -8.4034e-01, -2.2797e-01,  7.4925e-01,\n",
      "          3.8032e-01,  3.7505e-01,  1.2737e+00,  1.6927e-01, -6.6316e-01,\n",
      "          1.3727e+00, -6.5123e-01, -2.2249e-01,  1.4100e+00, -3.4060e-01,\n",
      "          2.9194e-02,  2.0781e+00, -8.5526e-01,  1.9907e+00, -1.7128e+00,\n",
      "         -1.8003e-01, -1.4265e-01,  9.7966e-01,  1.0307e+00,  2.7970e-01,\n",
      "          1.2845e+00,  1.8065e-01,  5.1817e-01,  2.0720e-01,  6.7168e-01,\n",
      "          1.6607e-01, -1.2493e-01,  9.6803e-01,  7.7274e-01,  1.6001e+00,\n",
      "          5.4745e-01,  3.6951e-02,  4.0909e-01,  5.8232e-01,  1.0385e+00,\n",
      "         -7.0367e-01,  1.1300e+00, -3.9949e-01,  1.3934e+00,  6.1723e-02,\n",
      "          3.7193e-02,  8.2905e-01,  5.0905e-01,  8.0758e-01,  1.1776e+00,\n",
      "          7.0223e-01,  2.1506e-01,  4.5844e-01, -2.1711e-01,  1.3794e+00,\n",
      "          4.5916e-01,  5.2153e-01,  1.0774e+00,  6.8702e-01,  8.8202e-01,\n",
      "          2.1893e-01,  2.5010e-01,  5.3817e-01,  1.6703e+00, -1.1451e+00,\n",
      "         -1.0049e+00, -6.5479e-01,  9.4313e-01,  8.9923e-01,  1.6839e+00,\n",
      "          3.1394e-01,  6.3477e-01,  9.7302e-01,  2.3854e-01, -2.7354e-02,\n",
      "          8.7709e-01,  1.1477e+00,  1.4418e+00,  8.7485e-01,  1.9179e-01,\n",
      "          1.1555e-02,  8.9494e-01,  1.0814e+00, -8.1473e-01,  2.3963e-01,\n",
      "         -8.8034e-01,  8.9404e-02, -1.2987e+00, -1.6353e+00,  1.0156e+00,\n",
      "          1.2589e+00,  3.4009e-01,  1.5684e-01,  1.3762e+00, -2.8546e-02,\n",
      "         -3.0272e-01,  1.0988e+00, -4.3023e-01,  1.5997e+00, -1.1875e+00,\n",
      "         -2.6349e-01,  4.6318e-01, -1.1222e+00,  1.6239e+00,  2.0657e-01,\n",
      "         -1.7220e+00, -9.6795e-01,  1.1630e-01,  5.5756e-01,  5.8749e-01,\n",
      "         -1.0888e+00,  5.9902e-01,  8.6217e-01,  1.3872e+00, -6.3618e-01,\n",
      "          1.0398e+00,  4.4585e-01, -5.6329e-01, -1.1463e+00, -1.5377e-01,\n",
      "          6.2271e-01,  1.4062e+00,  1.4290e+00,  8.6911e-01, -5.9607e-01,\n",
      "          1.4614e+00,  3.8119e-01,  2.7975e-01,  4.2996e-01,  5.6008e-01,\n",
      "          1.6526e+00,  7.9817e-01, -6.2549e-01,  4.9581e-01,  8.5625e-01,\n",
      "          1.3706e+00,  1.3365e+00,  2.1603e+00, -6.9061e-01, -4.6065e-01,\n",
      "          1.0930e+00, -6.7343e-01, -3.3521e-01, -2.5271e-01,  9.0249e-01,\n",
      "          1.5968e-01,  1.1660e+00,  1.3108e+00, -1.5995e-01, -2.6950e-01,\n",
      "          6.1522e-01, -2.3580e-02, -1.6779e-01,  1.4666e+00, -9.0579e-01,\n",
      "          9.7285e-01, -1.2833e+00,  1.2691e+00, -1.1990e+00, -2.5654e+00,\n",
      "          3.3822e-01,  1.5531e+00, -3.8273e-01,  7.2986e-02,  1.4325e+00,\n",
      "          9.0765e-01, -3.7611e-01,  1.2859e+00,  1.1828e+00,  1.0090e-01,\n",
      "          1.8708e-01, -6.7904e-01, -1.8561e-01, -1.1600e+00,  1.1037e-01,\n",
      "         -3.7404e-01,  3.8214e-01,  8.6856e-01,  1.9670e-01, -7.8247e-01,\n",
      "         -7.3912e-01,  1.1677e+00,  7.6293e-01,  1.9503e+00,  2.0332e+00,\n",
      "         -9.6771e-01, -4.4160e-01,  2.0431e+00,  9.2893e-01,  8.8399e-01,\n",
      "         -1.0432e-01, -3.6286e-01,  1.7462e+00, -1.0551e+00,  9.6964e-01,\n",
      "          1.2450e+00,  1.0949e+00,  7.5544e-01, -4.5579e-01, -1.9332e+00,\n",
      "         -2.9749e-01,  2.5623e-01,  3.6598e-01,  8.0135e-01,  3.3521e-02,\n",
      "         -3.5166e-01,  1.2049e+00, -6.1899e-01,  6.4500e-01, -3.2976e-01,\n",
      "         -9.7055e-01, -1.0427e+00, -6.3057e-01, -1.3635e-01,  1.4524e+00,\n",
      "         -3.7081e-01, -1.8205e-01,  3.5865e-01, -1.7811e+00,  2.8110e-02,\n",
      "         -3.8588e-01,  6.0588e-01,  1.8119e-01,  1.3913e-01,  1.1175e-01,\n",
      "         -1.2398e-02, -3.1652e-01,  2.8687e-02,  2.9749e-01, -3.8416e-01,\n",
      "         -6.7888e-01, -1.1590e+00,  5.0824e-01,  8.5566e-01, -1.3321e-01,\n",
      "         -1.2402e-02, -3.8288e-01, -5.4385e-01,  2.5094e-01,  6.1055e-01,\n",
      "         -6.6359e-01, -4.3283e-01, -5.1658e-01, -1.3590e-01, -1.0500e+00,\n",
      "          5.1827e-01,  2.5671e-01, -3.8271e-01, -8.2852e-01, -1.1503e+00,\n",
      "         -2.7306e-01,  8.3280e-01, -6.2599e-01,  7.0896e-01,  1.2087e-01,\n",
      "          1.4377e-02,  8.6525e-01, -2.6870e-01, -1.4661e-01, -2.1100e+00,\n",
      "          1.0394e+00, -1.5369e+00,  2.4562e-01,  4.2843e-02, -7.3210e-01,\n",
      "         -3.8624e-01, -9.9589e-02,  5.3079e-01, -5.1398e-01, -8.8362e-01,\n",
      "         -1.0776e+00, -1.9889e+00,  1.3673e+00, -3.0167e-01, -8.1807e-01,\n",
      "         -2.3608e-01, -1.2891e+00, -8.2196e-01, -1.9733e+00, -7.7082e-01,\n",
      "         -3.7096e-01,  1.2172e-01, -6.0469e-01,  1.2860e+00,  1.0090e+00]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prediction = model(data)\n",
    "print(f\"prediction is {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we use the model's prediction and the coresponding labels to calculate the error (`loss`). The next step is to `backpropagate` this error throught the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is -499.898681640625\n"
     ]
    }
   ],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "print(f\"loss is {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "now backpropagate the error (`loss`) throught the network using the method `backward`.\n",
    "This method `autograde` and stored the gradients for each model parameter in the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we load an optimizer, in this case SGD with a learning rate of 0.01 and momentum of 0.9. We register all the prameters of the model in this optimizer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, we call `.step()` to initiate gradient descent . The optimizer adjusts each parameter by it's gradient stored in `.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you have every thing you need to train your neuron network. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-ai-cookbooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
